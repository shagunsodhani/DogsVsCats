{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "from util import *\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_urls = {\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "}\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "#         out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = model_zoo.load_url(model_urls['resnet101'])\n",
    "        model_params = set(list(map(lambda x: x[0], model.named_parameters())))\n",
    "        new_state_dict = {\n",
    "            k:v for k, v in state_dict.items() if k in model_params\n",
    "        }\n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_mean = [0.485, 0.456, 0.406]\n",
    "image_std = [0.229, 0.224, 0.225]\n",
    "normalize = transforms.Normalize(mean=image_mean,\n",
    "                                     std=image_std)\n",
    "data_transforms = {\n",
    "    TRAIN : transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    VAL : transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    TEST : transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root=train_dir,\n",
    "                           transform = data_transforms[TRAIN])\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True,\n",
    "                         num_workers = num_workers)\n",
    "\n",
    "val_dataset = ImageFolder(root=val_dir,\n",
    "                           transform = data_transforms[VAL])\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True,\n",
    "                         num_workers = num_workers)\n",
    "\n",
    "test_dataset = ImageFolder(root=test_dir,\n",
    "                           transform = data_transforms[TEST])\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True,\n",
    "                         num_workers = num_workers)\n",
    "\n",
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array(image_mean)\n",
    "    std = np.array(image_std)\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    plt.imsave(\"final_data_augmentation.png\", inp)\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "batch_to_view = next(iter(train_loader))\n",
    "inputs, classes = batch_to_view[0][:4], batch_to_view[1][:4]\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(model_name=RESNET50, use_cuda=True):\n",
    "    model = resnet101(pretrained=True)\n",
    "    model.fc = torch.nn.Linear(in_features=2048, out_features=2)\n",
    "    if(use_cuda):\n",
    "        return model.cuda()\n",
    "    else:\n",
    "        return model.cuda()\n",
    "\n",
    "def get_loss_fn(use_cuda=True):\n",
    "    if(use_cuda):\n",
    "        return torch.nn.CrossEntropyLoss().cuda()\n",
    "    else:\n",
    "        return torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def get_optimizer(model, learning_rate):\n",
    "    return torch.optim.Adam(\n",
    "        model.fc.parameters(), learning_rate\n",
    "    )\n",
    "\n",
    "def get_lr_scheduler(optimizer, step_size = 10, gamma=1):\n",
    "    return torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "def persist_model(model_state, save_path):\n",
    "    torch.save(model_state, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelMetric():\n",
    "    '''class to track the avergae accuracy for the model with different datasets\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.correct_count = 0.0\n",
    "        self.total_count = 0.0\n",
    "        self.total_loss = 0.0\n",
    "    \n",
    "    def update(self, correct_count, count, loss):\n",
    "        self.correct_count+=correct_count\n",
    "        self.total_count+=count\n",
    "        if(use_cuda):\n",
    "            loss = loss.cpu().data.numpy()[0]\n",
    "        else:\n",
    "            loss = loss.data.numpy()[0]\n",
    "        self.total_loss+=loss*count\n",
    "    \n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        return self.correct_count/self.total_count\n",
    "    \n",
    "    @property\n",
    "    def average_loss(self):\n",
    "        return self.total_loss/self.total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, labels):\n",
    "    \"\"\"Computes the number of correct matches in y_pred\"\"\"\n",
    "    _, predictions = y_pred.topk(k = 1, dim=1)\n",
    "    batch_size = labels.size(0)\n",
    "    correct_count = torch.sum(predictions.eq(labels.view(-1, 1).expand_as(predictions)))\n",
    "    if(use_cuda):\n",
    "        correct_count = correct_count.cpu().data.numpy()[0]\n",
    "    else:\n",
    "        correct_count = correct_count.data.numpy()[0]\n",
    "    total_count = labels.shape[0]\n",
    "    return (correct_count, total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val(model, val_loader, loss_fn, metric):\n",
    "    \n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for (images, labels) in val_loader:\n",
    "        images = torch.autograd.Variable(images, volatile=True)\n",
    "        labels = torch.autograd.Variable(labels, volatile=True)\n",
    "        if(use_cuda):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        predictions = model(images)\n",
    "        predictions_loss = loss_fn(predictions, labels)\n",
    "        num_correct_predictions, num_total_predictions = compute_accuracy(predictions, labels)\n",
    "        metric.update(num_correct_predictions, num_total_predictions, predictions_loss)\n",
    "        \n",
    "    print(\"Validation Accuracy = {}, Validation Loss = {}, Time Taken = {} seconds\".format(\n",
    "        metric.accuracy,\n",
    "        metric.average_loss,\n",
    "        time.time() - start_time\n",
    "    ))\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, optimizer, metric = None):\n",
    "    running_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for (images, labels) in train_loader:\n",
    "        images = torch.autograd.Variable(images)\n",
    "        labels = torch.autograd.Variable(labels)\n",
    "        if(use_cuda):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        predictions = model(images)\n",
    "        predictions_loss = loss_fn(predictions, labels)\n",
    "        num_correct_predictions, num_total_predictions = compute_accuracy(predictions, labels)\n",
    "        metric.update(num_correct_predictions, num_total_predictions, predictions_loss)\n",
    "        optimizer.zero_grad()\n",
    "        predictions_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += predictions_loss.data[0]\n",
    "        \n",
    "    print(\"Training Accuracy = {}, Training Loss = {}, Time Taken = {} seconds\".format(\n",
    "        train_metric.accuracy,\n",
    "        train_metric.average_loss,\n",
    "        time.time() - start_time\n",
    "    ))\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model(model_name=model_name, \n",
    "                 use_cuda=use_cuda)\n",
    "loss_fn = get_loss_fn(use_cuda=use_cuda)\n",
    "optimizer = get_optimizer(model=model, \n",
    "                          learning_rate=learning_rate)\n",
    "lr_scheduler = get_lr_scheduler(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = 0.6102926829268293, Training Loss = 0.6533627269209885, Time Taken = 186.75403571128845 seconds\n",
      "Validation Accuracy = 0.614, Validation Loss = 0.6414008043289184, Time Taken = 17.024537801742554 seconds\n",
      "Training Accuracy = 0.6268292682926829, Training Loss = 0.642121183075556, Time Taken = 171.85565614700317 seconds\n",
      "Validation Accuracy = 0.6188, Validation Loss = 0.6419092105388642, Time Taken = 15.967804193496704 seconds\n",
      "Training Accuracy = 0.6352520325203252, Training Loss = 0.6348179772229698, Time Taken = 173.24706315994263 seconds\n",
      "Validation Accuracy = 0.6301333333333333, Validation Loss = 0.6332199411710103, Time Taken = 24.93998885154724 seconds\n",
      "Training Accuracy = 0.6424390243902439, Training Loss = 0.6291671537655156, Time Taken = 179.59887599945068 seconds\n",
      "Validation Accuracy = 0.6371, Validation Loss = 0.6283570964574814, Time Taken = 15.367051124572754 seconds\n",
      "Training Accuracy = 0.6478243902439025, Training Loss = 0.6246851948435713, Time Taken = 172.68958282470703 seconds\n",
      "Validation Accuracy = 0.64304, Validation Loss = 0.6242542025947571, Time Taken = 15.311936616897583 seconds\n",
      "Training Accuracy = 0.6527479674796748, Training Loss = 0.6208261667073257, Time Taken = 172.5800302028656 seconds\n",
      "Validation Accuracy = 0.649, Validation Loss = 0.6201885596195856, Time Taken = 15.739771842956543 seconds\n",
      "Training Accuracy = 0.6569407665505227, Training Loss = 0.6173631969212655, Time Taken = 170.1712827682495 seconds\n",
      "Validation Accuracy = 0.6536, Validation Loss = 0.6167774506705148, Time Taken = 16.41205096244812 seconds\n",
      "Training Accuracy = 0.6605426829268293, Training Loss = 0.6144385653370764, Time Taken = 170.94607591629028 seconds\n",
      "Validation Accuracy = 0.65795, Validation Loss = 0.6136158263325692, Time Taken = 19.8434157371521 seconds\n",
      "Training Accuracy = 0.6640271002710028, Training Loss = 0.6117111314486682, Time Taken = 184.9717960357666 seconds\n",
      "Validation Accuracy = 0.6612444444444444, Validation Loss = 0.610963888030582, Time Taken = 15.583875179290771 seconds\n",
      "Training Accuracy = 0.6667804878048781, Training Loss = 0.6093082922830815, Time Taken = 171.29145646095276 seconds\n",
      "Validation Accuracy = 0.66408, Validation Loss = 0.6084978676891327, Time Taken = 15.2233407497406 seconds\n",
      "Training Accuracy = 0.6694013303769402, Training Loss = 0.607212866874069, Time Taken = 170.8934407234192 seconds\n",
      "Validation Accuracy = 0.6670909090909091, Validation Loss = 0.6064965544007042, Time Taken = 16.410427808761597 seconds\n",
      "Training Accuracy = 0.6717723577235772, Training Loss = 0.6052573758916157, Time Taken = 171.04877042770386 seconds\n",
      "Validation Accuracy = 0.6688, Validation Loss = 0.6049369111935298, Time Taken = 15.913269519805908 seconds\n",
      "Training Accuracy = 0.6740863039399625, Training Loss = 0.6033793284405463, Time Taken = 185.72928667068481 seconds\n",
      "Validation Accuracy = 0.6712, Validation Loss = 0.6032254022965065, Time Taken = 23.968491315841675 seconds\n",
      "Training Accuracy = 0.6763658536585366, Training Loss = 0.601543802287105, Time Taken = 169.68100595474243 seconds\n",
      "Validation Accuracy = 0.6727142857142857, Validation Loss = 0.6022018700940268, Time Taken = 15.259332656860352 seconds\n",
      "Training Accuracy = 0.6784715447154471, Training Loss = 0.59979267266824, Time Taken = 175.48719143867493 seconds\n",
      "Validation Accuracy = 0.6746133333333333, Validation Loss = 0.6007574749978384, Time Taken = 15.190481662750244 seconds\n",
      "Training Accuracy = 0.6801432926829268, Training Loss = 0.5982717617626597, Time Taken = 170.4601149559021 seconds\n",
      "Validation Accuracy = 0.676325, Validation Loss = 0.5991354346245528, Time Taken = 15.303332328796387 seconds\n",
      "Training Accuracy = 0.6817761836441893, Training Loss = 0.5967996235759221, Time Taken = 175.55539083480835 seconds\n",
      "Validation Accuracy = 0.6777176470588235, Validation Loss = 0.5977163537221797, Time Taken = 15.88806676864624 seconds\n",
      "Training Accuracy = 0.6833739837398374, Training Loss = 0.5953912629528744, Time Taken = 178.81355142593384 seconds\n",
      "Validation Accuracy = 0.6792, Validation Loss = 0.5965205836216608, Time Taken = 16.129104614257812 seconds\n",
      "Training Accuracy = 0.6848189987163029, Training Loss = 0.5941060422158517, Time Taken = 172.179771900177 seconds\n",
      "Validation Accuracy = 0.6804210526315789, Validation Loss = 0.5952809839675302, Time Taken = 15.230842113494873 seconds\n",
      "Training Accuracy = 0.6861439024390243, Training Loss = 0.5928155862369189, Time Taken = 173.41072940826416 seconds\n",
      "Validation Accuracy = 0.68192, Validation Loss = 0.5940295511460304, Time Taken = 16.24379801750183 seconds\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = 1e6\n",
    "train_metric = ModelMetric()\n",
    "val_metric = ModelMetric()\n",
    "running_loss = 0\n",
    "early_stopping_counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    train_metric = train(model = model,\n",
    "         train_loader = train_loader,\n",
    "         loss_fn = loss_fn, \n",
    "         optimizer = optimizer,\n",
    "         metric = train_metric)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    val_metric = val(model = model,\n",
    "                          val_loader = val_loader,\n",
    "                          loss_fn = loss_fn,\n",
    "                        metric = val_metric)\n",
    "    val_accuracy = val_metric.accuracy\n",
    "    val_loss = val_metric.average_loss\n",
    "    if(val_loss < best_val_loss):\n",
    "        best_val_loss = val_loss\n",
    "        model_state = {\n",
    "            EPOCH: epoch+1,\n",
    "            STATE_DICT: model.state_dict(),\n",
    "            VAL_ACCURACY: val_accuracy,\n",
    "            VAL_LOSS: val_loss\n",
    "        }\n",
    "        save_path = \"/u/sodhanis/projects/DogsVsCats/model/checkpoint_epoch_\"\\\n",
    "        +str(epoch)+\"_val_accuracy_\"+str(int(val_accuracy*1e4))+\"_val_loss_\"+str(int(val_loss*1e4))+\".path.tar\"\n",
    "        persist_model(model_state=model_state,\n",
    "                     save_path = save_path)\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter+=1\n",
    "        if(early_stopping_counter == early_stopping_criteria):\n",
    "            break\n",
    "            print(\"Early stopping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.7115, Test Loss = 0.5726364305019379, Time Taken = 20.62707495689392 seconds\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader, loss_fn, metric):\n",
    "    \n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for (images, labels) in test_loader:\n",
    "        images = torch.autograd.Variable(images, volatile=True)\n",
    "        labels = torch.autograd.Variable(labels, volatile=True)\n",
    "        if(use_cuda):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        predictions = model(images)\n",
    "        predictions_loss = loss_fn(predictions, labels)\n",
    "        num_correct_predictions, num_total_predictions = compute_accuracy(predictions, labels)\n",
    "        metric.update(num_correct_predictions, num_total_predictions, predictions_loss)\n",
    "        \n",
    "    print(\"Test Accuracy = {}, Test Loss = {}, Time Taken = {} seconds\".format(\n",
    "        metric.accuracy,\n",
    "        metric.average_loss,\n",
    "        time.time() - start_time\n",
    "    ))\n",
    "    return metric\n",
    "test_metric = ModelMetric()\n",
    "test_metric = test(model, test_loader, loss_fn, test_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=2):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    for (i, (images, labels)) in enumerate(val_loader):\n",
    "        images = torch.autograd.Variable(images)\n",
    "        labels = torch.autograd.Variable(labels)\n",
    "        if(use_cuda):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        for j in range(images.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "            imshow(images.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                model.train(mode=was_training)\n",
    "                return\n",
    "    model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = model.parameters()\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Total number of parameters in {} model = {}\".format(model_name, params))\n",
    "\n",
    "model_parameters = model.fc.parameters()\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Total number of trainable parameters in the finetuned {} model = {}\".format(model_name, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_model(model, num_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
